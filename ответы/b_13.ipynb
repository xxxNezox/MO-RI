{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Опишите  общую  схему  поиска  изображений,  включая  роль  дескрипторов \n",
    "изображений  и  методы  ранжирования  результатов.  Как  семантическое \n",
    "хеширование и алгоритм GIST используются для улучшения эффективности и \n",
    "точности  поиска?  В  чем  заключается  вклад  этих  технологий  в  улучшение \n",
    "доступа к большим коллекциям изображений? \n",
    "2.  Реализуйте алгоритм сшивания изображений для создания панорамы из серии \n",
    "фотографий. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение признаков – представление изображения в числовом виде с помощью дескрипторов.\n",
    "Индексирование – хранение и организация этих представлений для быстрого поиска.\n",
    "Ранжирование результатов – сортировка изображений по степени сходства с запросом.\n",
    "\n",
    "Дескрипторы – это числовые представления изображений, извлекаемые с помощью алгоритмов компьютерного зрения.\n",
    "\n",
    "Глубокая нейросеть обучается так, чтобы похожие изображения получали близкие хеш-коды.\n",
    "Поиск по хеш-кодам выполняется за O(1) операций, что намного быстрее, чем сравнение полных дескрипторов.\n",
    "\n",
    "GIST:\n",
    "Разделяет изображение на регионы.\n",
    "Применяет фильтры для извлечения информации о текстуре и ориентации.\n",
    "Формирует компактный вектор признаков (~512 значений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 101,  52],\n",
       "        [181, 102,  53],\n",
       "        [183, 104,  55],\n",
       "        ...,\n",
       "        [215, 143, 101],\n",
       "        [215, 143, 101],\n",
       "        [211, 139,  99]],\n",
       "\n",
       "       [[184, 105,  56],\n",
       "        [186, 107,  58],\n",
       "        [187, 108,  59],\n",
       "        ...,\n",
       "        [212, 140, 100],\n",
       "        [212, 140, 100],\n",
       "        [214, 142, 102]],\n",
       "\n",
       "       [[186, 107,  58],\n",
       "        [188, 109,  60],\n",
       "        [189, 110,  61],\n",
       "        ...,\n",
       "        [216, 144, 104],\n",
       "        [216, 144, 104],\n",
       "        [217, 145, 105]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        ...,\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47]],\n",
       "\n",
       "       [[ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        ...,\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47]],\n",
       "\n",
       "       [[ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        [ 74,  31,  16],\n",
       "        ...,\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47],\n",
       "        [135,  71,  47]]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_panorama(image1_path, image2_path, output_path=\"panorama.jpg\"):\n",
    "    # Загружаем изображения\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "\n",
    "    # Инициализируем ORB детектор\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Находим ключевые точки и дескрипторы с помощью ORB\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(image2, None)\n",
    "\n",
    "    # Создаем объект BFMatcher и совершаем сопоставление дескрипторов\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Сортируем сопоставления по расстоянию (лучшие сопоставления первые)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Извлекаем координаты соответствующих ключевых точек для сопоставленных дескрипторов\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Находим матрицу гомографии\n",
    "    H, _ = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Применяем преобразование гомографии к изображению 1\n",
    "    height, width, channels = image2.shape\n",
    "    panorama = cv2.warpPerspective(image1, H, (width * 2, height))\n",
    "\n",
    "    # Копируем изображение 2 в панорамное изображение\n",
    "    panorama[0 : image2.shape[0], 0 : image2.shape[1]] = image2\n",
    "\n",
    "    # Сохраняем панорамное изображение\n",
    "    cv2.imwrite(output_path, panorama)\n",
    "\n",
    "    return image2\n",
    "\n",
    "\n",
    "# Пример использования:\n",
    "create_panorama(\"./data/panorama02.jpg\", \"./data/panorama01.jpg\", \"./data/output_panorama.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
